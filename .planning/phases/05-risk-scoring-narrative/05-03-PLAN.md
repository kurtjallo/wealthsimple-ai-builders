---
phase: 05-risk-scoring-narrative
plan: 03
type: execute
wave: 2
depends_on: ["05-01"]
files_modified:
  - src/agents/case-narrator/index.ts
  - src/agents/case-narrator/evidence-builder.ts
  - src/agents/case-narrator/prompt-templates.ts
autonomous: true

must_haves:
  truths:
    - "Case Narrator agent generates a 2-3 paragraph narrative summary suitable for regulatory review"
    - "Every claim in the narrative has a linked evidence reference (e.g., DOC-001, OFAC-SDN-001)"
    - "Narrative follows structured format: Summary, Key Findings, Risk Factors, Recommended Action"
    - "Narrative uses clear, professional language — not AI-sounding filler"
  artifacts:
    - path: "src/agents/case-narrator/evidence-builder.ts"
      provides: "Evidence link construction from agent signals"
      exports: ["buildEvidenceLinks", "buildKeyFindings", "buildRiskFactors"]
    - path: "src/agents/case-narrator/prompt-templates.ts"
      provides: "System prompt and narrative generation templates"
      exports: ["NARRATOR_SYSTEM_PROMPT", "buildNarrativePrompt"]
    - path: "src/agents/case-narrator/index.ts"
      provides: "Case Narrator agent definition using Claude Agent SDK"
      exports: ["caseNarratorAgent", "runCaseNarrator"]
  key_links:
    - from: "src/agents/case-narrator/evidence-builder.ts"
      to: "src/types/risk.ts"
      via: "imports signal types to extract evidence"
      pattern: "import.*Signal.*from.*types/risk"
    - from: "src/agents/case-narrator/evidence-builder.ts"
      to: "src/types/narrative.ts"
      via: "returns EvidenceLink, KeyFinding, RiskFactor types"
      pattern: "import.*EvidenceLink.*from.*types/narrative"
    - from: "src/agents/case-narrator/index.ts"
      to: "src/agents/case-narrator/prompt-templates.ts"
      via: "uses prompt templates for agent system prompt"
      pattern: "import.*NARRATOR_SYSTEM_PROMPT"
---

<objective>
Build the Case Narrator agent that generates human-readable risk assessment narratives with linked evidence for every claim.

Purpose: ORCH-06 — the Case Narrator is the communication layer of the system. It transforms raw scores and signals into a professional narrative that a compliance officer can read, understand, and act on. Every claim must be traceable to specific evidence, meeting regulatory audit requirements.

Output: A working Case Narrator agent that takes a RiskAssessment + agent signals as input and produces a structured CaseNarrative with evidence-linked findings.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/05-risk-scoring-narrative/05-01-SUMMARY.md
@src/types/risk.ts
@src/types/narrative.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build evidence extraction and key findings builder</name>
  <files>src/agents/case-narrator/evidence-builder.ts</files>
  <action>
Create `src/agents/case-narrator/evidence-builder.ts` — pure logic for extracting structured evidence from agent signals. No LLM calls.

**Function 1: `buildEvidenceLinks(signals, riskScore): EvidenceLink[]`**

Generate evidence links from each agent's output:

From DocumentSignal:
- ID format: `DOC-{NNN}` (e.g., DOC-001)
- type: 'document'
- source: `"{documentType} processed via OCR"` (e.g., "Passport processed via OCR")
- description: Include extraction confidence and validity status
  - Valid + high confidence: "Document verified as valid with {confidence}% extraction confidence. {fieldsExtracted} fields successfully extracted."
  - Invalid/uncertain: "Document flagged as {validity}. {fieldsMissing.length} fields could not be extracted: {fieldsMissing.join(', ')}."
- confidence: signal.extractionConfidence

From IdentitySignal:
- ID format: `ID-VERIFY-{NNN}` (e.g., ID-VERIFY-001)
- type: 'identity_verification'
- source: "Identity cross-reference verification"
- description: Include verification status and any discrepancies
  - Verified: "Identity verified with {matchConfidence}% confidence. Fields verified: {fieldsVerified.join(', ')}."
  - Discrepancies: "Identity verification found {discrepancies.length} discrepancy(ies): {describe each}."
- confidence: signal.matchConfidence

From SanctionsSignal:
- ID format: `SANC-{LIST}-{NNN}` (e.g., SANC-OFAC-001)
- One evidence link per screened list
- type: 'sanctions_list'
- source: `"{listName} checked {signal.timestamp}"` (e.g., "OFAC SDN List checked 2026-02-23")
- description:
  - Clear: "No matches found on {listName}."
  - Potential match: "Potential match detected: {matchedName} (fuzzy score: {fuzzyScore}, match type: {matchType})."
  - Confirmed match: "CONFIRMED MATCH: {matchedName} on {listName}."
- confidence: based on match status

From PEPSignal:
- ID format: `PEP-{NNN}` (e.g., PEP-001)
- type: 'pep_screening'
- source: signal.source or "PEP database screening"
- description:
  - Clear: "No PEP status identified."
  - Potential: "Potential PEP identification with {confidence}% confidence."
  - Confirmed: "Confirmed PEP: {pepCategory} classification."
- confidence: signal.confidence

**Function 2: `buildKeyFindings(signals, riskScore, evidenceLinks): KeyFinding[]`**

Generate key findings — one per agent category:

1. Document finding:
   - category: 'document'
   - summary: Based on documentValidity and extractionConfidence
   - evidenceLinks: [relevant DOC-* IDs]
   - riskImpact: 'positive' if valid + high confidence, 'neutral' if uncertain, 'negative' if invalid

2. Identity finding:
   - category: 'identity'
   - summary: Based on verificationStatus
   - evidenceLinks: [relevant ID-VERIFY-* IDs]
   - riskImpact: based on verification status

3. Sanctions finding:
   - category: 'sanctions'
   - summary: Based on matchStatus. If confirmed match, this is CRITICAL.
   - evidenceLinks: [relevant SANC-* IDs]
   - riskImpact: 'positive' if clear, 'negative' if potential, 'critical' if confirmed

4. PEP finding:
   - category: 'pep'
   - summary: Based on pepStatus
   - evidenceLinks: [relevant PEP-* IDs]
   - riskImpact: based on PEP status

**Function 3: `buildRiskFactors(signals, riskScore): RiskFactor[]`**

Generate risk factors — only for signals that indicate elevated risk (don't list non-issues as risk factors):

Rules:
- Only include factors where component score > 25 (above "low" threshold)
- For each elevated component, create a RiskFactor with:
  - factor: Clear label (e.g., "Elevated sanctions risk", "Identity verification incomplete")
  - severity: Map from component score ranges (0-25: skip, 26-50: 'medium', 51-75: 'high', 76-100: 'critical')
  - description: Specific explanation of what caused the elevated score
  - evidenceLinks: Relevant evidence IDs

- If no risk factors exist (all components low), return empty array.

All functions are pure and synchronous. Import types from `@/types/risk` and `@/types/narrative`.
  </action>
  <verify>
1. `npx tsc --noEmit src/agents/case-narrator/evidence-builder.ts` — compiles
2. Verify evidence ID format consistency (DOC-001, ID-VERIFY-001, SANC-OFAC-001, PEP-001)
3. Verify that buildRiskFactors returns empty array for all-clear signals
4. Verify that confirmed sanctions match generates a 'critical' risk factor
  </verify>
  <done>Evidence builder generates structured EvidenceLink[], KeyFinding[], and RiskFactor[] from raw agent signals. Every claim is traceable via evidence IDs. Risk factors only appear for genuinely elevated risks (score > 25).</done>
</task>

<task type="auto">
  <name>Task 2: Create prompt templates and Case Narrator agent</name>
  <files>src/agents/case-narrator/prompt-templates.ts, src/agents/case-narrator/index.ts</files>
  <action>
**Part A: Create `src/agents/case-narrator/prompt-templates.ts`**

Define the system prompt and narrative generation prompt for the Case Narrator:

```typescript
export const NARRATOR_SYSTEM_PROMPT = `You are the Case Narrator agent in a KYC/AML compliance system. You generate professional, regulatory-grade risk assessment narratives.

Your narratives will be read by compliance officers and may be reviewed by regulators. They must be:
- Clear and direct (no hedging, no AI filler phrases like "it is important to note")
- Factual — every claim references specific evidence via bracket notation [DOC-001], [SANC-OFAC-001]
- Structured: Summary → Key Findings → Risk Factors → Recommended Action
- Professional tone suitable for FINTRAC/regulatory filing

Evidence references MUST use the exact IDs provided in the evidence links. Do not fabricate evidence IDs.

For the Summary section:
- 2-3 paragraphs maximum
- Lead with the risk category and composite score
- Mention each agent's key finding in summary form
- If auto-escalation triggered, state this prominently in the first paragraph

For each Key Finding, reference the evidence: "Passport verified with high confidence [DOC-001]"

For Risk Factors, explain WHY something is a risk, not just that it is one.

For Recommended Action, be specific:
- Low risk (0-25): "Routine approval recommended"
- Medium risk (26-50): "Standard review by compliance officer recommended"
- High risk (51-75): "Enhanced due diligence required before proceeding"
- Critical risk (76-100): "Immediate escalation to senior compliance officer required"`;
```

Create `buildNarrativePrompt(riskAssessment, keyFindings, riskFactors, evidenceLinks)` function that constructs the user message for the LLM:

```typescript
export function buildNarrativePrompt(
  riskAssessment: RiskAssessment,
  keyFindings: KeyFinding[],
  riskFactors: RiskFactor[],
  evidenceLinks: EvidenceLink[]
): string {
  // Build a structured prompt that includes:
  // 1. The case ID and risk score summary
  // 2. All evidence links (so the LLM knows what IDs to reference)
  // 3. Key findings (pre-built from evidence-builder)
  // 4. Risk factors (pre-built from evidence-builder)
  // 5. Instruction to generate the narrative summary text
  //
  // The LLM generates the SUMMARY text and the RECOMMENDED ACTION reasoning.
  // Key findings and risk factors are already structured — the LLM just enriches them with natural language.
}
```

The prompt should present evidence links as a reference table:
```
Available Evidence:
- [DOC-001]: Passport processed via OCR — verified, 95% confidence
- [SANC-OFAC-001]: OFAC SDN List checked 2026-02-23 — no match
...
```

**Part B: Create `src/agents/case-narrator/index.ts`**

Define the Case Narrator agent using Claude Agent SDK:

Agent configuration:
- name: "Case Narrator"
- model: "claude-sonnet-4-6" (needs strong writing + reasoning for narrative quality)
- system prompt: NARRATOR_SYSTEM_PROMPT
- No tools needed — this agent generates text, not computes

Export:
- `caseNarratorAgent` — the agent definition
- `runCaseNarrator(riskAssessment: RiskAssessment, signals: { document, identity, sanctions, pep }): Promise<CaseNarrative>` — convenience function that:
  1. Calls `buildEvidenceLinks(signals, riskAssessment.score)` to get evidence
  2. Calls `buildKeyFindings(signals, riskAssessment.score, evidenceLinks)` to get findings
  3. Calls `buildRiskFactors(signals, riskAssessment.score)` to get risk factors
  4. Calls `buildNarrativePrompt(...)` to construct the LLM prompt
  5. Invokes the agent with the prompt
  6. Parses the LLM response to extract the summary text
  7. Determines recommended action based on risk category (low → approve, medium → review, high → escalate, critical → escalate with immediate urgency)
  8. Returns complete CaseNarrative object with:
     - id: generated UUID
     - caseId: from riskAssessment
     - riskAssessmentId: from riskAssessment.id
     - summary: from LLM output
     - keyFindings, riskFactors, evidenceLinks: from evidence-builder
     - recommendedAction: derived from risk category + LLM reasoning
     - generatedAt: ISO timestamp
     - generatedBy: 'case_narrator_agent'
     - modelUsed: 'claude-sonnet-4-6'

Follow Claude Agent SDK patterns established in Phase 2. The agent uses the LLM purely for narrative generation — all structured data comes from the evidence-builder module.
  </action>
  <verify>
1. `npx tsc --noEmit src/agents/case-narrator/index.ts` — compiles
2. System prompt does NOT contain AI filler phrases
3. `runCaseNarrator` returns a complete CaseNarrative with all required fields
4. Evidence links in prompt use consistent ID formats matching evidence-builder output
5. Recommended action maps correctly: low→approve, medium→review, high→escalate, critical→escalate(immediate)
  </verify>
  <done>Case Narrator agent generates structured, evidence-linked narratives. Pre-built evidence and findings ensure accuracy; LLM generates only the prose summary. Every claim references evidence IDs. Output is regulatory-grade professional language.</done>
</task>

</tasks>

<verification>
1. All three files compile without TypeScript errors
2. Evidence builder generates correct IDs (DOC-001, ID-VERIFY-001, SANC-OFAC-001, PEP-001)
3. System prompt enforces evidence-linked claims and professional tone
4. Agent follows Claude Agent SDK patterns from Phase 2
5. Narrative structure: Summary → Key Findings → Risk Factors → Recommended Action
6. All-clear case produces a simple "routine approval" narrative
7. Critical case produces an escalation narrative with specific evidence
</verification>

<success_criteria>
- Case Narrator generates 2-3 paragraph summary with evidence bracket notation [DOC-001]
- Key findings, risk factors, and evidence links are structured (not free-text)
- Recommended action derived from risk category with specific guidance
- Agent can be invoked via `runCaseNarrator()` and returns complete CaseNarrative
- Professional, regulatory-suitable language (no AI filler)
</success_criteria>

<output>
After completion, create `.planning/phases/05-risk-scoring-narrative/05-03-SUMMARY.md`
</output>
