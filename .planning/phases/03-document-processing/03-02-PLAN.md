---
phase: 03-document-processing
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - supabase/migrations/003_documents_storage.sql
  - src/lib/supabase/documents.ts
  - src/lib/supabase/storage.ts
autonomous: true

must_haves:
  truths:
    - "Documents table exists in Supabase with columns for OCR output, extracted data, and confidence"
    - "Document files can be uploaded to Supabase Storage and a signed URL retrieved"
    - "Document records can be created, read, and updated in the database"
  artifacts:
    - path: "supabase/migrations/003_documents_storage.sql"
      provides: "Database migration for documents table and storage bucket"
      contains: "CREATE TABLE"
    - path: "src/lib/supabase/documents.ts"
      provides: "CRUD operations for documents table"
      contains: "createDocument"
    - path: "src/lib/supabase/storage.ts"
      provides: "File upload/download helpers for Supabase Storage"
      contains: "uploadDocument"
  key_links:
    - from: "src/lib/supabase/documents.ts"
      to: "@supabase/supabase-js"
      via: "Supabase client"
      pattern: "supabase.*from.*documents"
    - from: "src/lib/supabase/storage.ts"
      to: "@supabase/supabase-js"
      via: "Supabase storage client"
      pattern: "supabase.*storage"
---

<objective>
Create the Supabase database schema for document storage and build the data access layer for document CRUD operations and file uploads.

Purpose: Documents need persistent storage for both the raw files (images/PDFs) and their extracted data. This plan creates the database schema and helper functions that the document processor agent will use.
Output: SQL migration for documents table, TypeScript helpers for document CRUD and file storage operations.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/types/index.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create documents database migration and storage bucket setup</name>
  <files>supabase/migrations/003_documents_storage.sql</files>
  <action>
    1. Create directory `supabase/migrations/` if it doesn't exist.

    2. Create `supabase/migrations/003_documents_storage.sql`:

    ```sql
    -- Documents table for storing document metadata and extracted data
    -- Numbering assumes 001 = cases table, 002 = agent_runs table (from Phase 1/2)
    -- If those don't exist yet, this migration is still self-contained.

    CREATE TABLE IF NOT EXISTS documents (
      id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
      case_id UUID NOT NULL,
      type TEXT NOT NULL CHECK (type IN ('passport', 'drivers_license', 'utility_bill', 'bank_statement', 'corporate_doc')),
      file_name TEXT NOT NULL,
      file_path TEXT, -- Path in Supabase Storage bucket
      file_url TEXT, -- Signed URL (populated after upload)
      ocr_raw_text TEXT, -- Raw OCR markdown output
      extracted_data JSONB, -- Structured extracted data (PassportData, etc.)
      overall_confidence REAL, -- 0.0 to 1.0
      processing_status TEXT NOT NULL DEFAULT 'pending' CHECK (processing_status IN ('pending', 'processing', 'completed', 'failed')),
      processing_error TEXT, -- Error message if processing failed
      processing_time_ms INTEGER, -- How long OCR + extraction took
      warnings TEXT[], -- Array of warning strings
      created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
      updated_at TIMESTAMPTZ NOT NULL DEFAULT NOW()
    );

    -- Index for fast lookups by case
    CREATE INDEX IF NOT EXISTS idx_documents_case_id ON documents(case_id);
    CREATE INDEX IF NOT EXISTS idx_documents_type ON documents(type);
    CREATE INDEX IF NOT EXISTS idx_documents_processing_status ON documents(processing_status);

    -- Auto-update updated_at timestamp
    CREATE OR REPLACE FUNCTION update_documents_updated_at()
    RETURNS TRIGGER AS $$
    BEGIN
      NEW.updated_at = NOW();
      RETURN NEW;
    END;
    $$ LANGUAGE plpgsql;

    CREATE TRIGGER documents_updated_at
      BEFORE UPDATE ON documents
      FOR EACH ROW
      EXECUTE FUNCTION update_documents_updated_at();

    -- Note: Supabase Storage bucket 'documents' should be created via the
    -- Supabase dashboard or CLI. The bucket stores the actual document files.
    -- Storage policies should allow authenticated uploads and signed URL generation.
    ```

    IMPORTANT: This migration is designed to be run via the Supabase dashboard SQL editor or `supabase db push`. It uses `IF NOT EXISTS` for safety. The case_id column references cases but does NOT add a foreign key constraint — this allows flexible ordering of phase execution (Phase 3 may run before the cases table exists in dev).
  </action>
  <verify>
    - File `supabase/migrations/003_documents_storage.sql` exists
    - SQL is valid (CREATE TABLE, CREATE INDEX, CREATE TRIGGER)
    - Table has columns: id, case_id, type, file_name, file_path, file_url, ocr_raw_text, extracted_data, overall_confidence, processing_status, processing_error, processing_time_ms, warnings, created_at, updated_at
    - CHECK constraints on type and processing_status columns
  </verify>
  <done>
    Database migration created with documents table, indexes, and auto-updating timestamp trigger. Ready to be applied to Supabase.
  </done>
</task>

<task type="auto">
  <name>Task 2: Build document CRUD helpers and storage upload functions</name>
  <files>src/lib/supabase/documents.ts, src/lib/supabase/storage.ts, src/lib/supabase/client.ts</files>
  <action>
    1. First, create `src/lib/supabase/client.ts` if it doesn't already exist — a shared Supabase client for server-side operations:

    ```typescript
    import { createClient } from '@supabase/supabase-js';

    export function getSupabaseClient() {
      const url = process.env.NEXT_PUBLIC_SUPABASE_URL;
      const key = process.env.SUPABASE_SERVICE_ROLE_KEY || process.env.NEXT_PUBLIC_SUPABASE_ANON_KEY;
      if (!url || !key) {
        throw new Error('Supabase URL and key must be set in environment variables');
      }
      return createClient(url, key);
    }
    ```

    2. Create `src/lib/supabase/storage.ts` — file upload/download helpers:

    ```typescript
    import { getSupabaseClient } from './client';

    const BUCKET_NAME = 'documents';

    /**
     * Upload a document file to Supabase Storage.
     * Returns the storage path (not a URL).
     */
    export async function uploadDocument(
      caseId: string,
      fileName: string,
      fileBuffer: Buffer,
      contentType: string
    ): Promise<{ path: string }> {
      const supabase = getSupabaseClient();
      const storagePath = `${caseId}/${Date.now()}-${fileName}`;

      const { data, error } = await supabase.storage
        .from(BUCKET_NAME)
        .upload(storagePath, fileBuffer, {
          contentType,
          upsert: false,
        });

      if (error) {
        throw new Error(`Failed to upload document: ${error.message}`);
      }

      return { path: data.path };
    }

    /**
     * Get a signed URL for a document in storage.
     * URL expires after the specified duration (default 1 hour).
     */
    export async function getDocumentSignedUrl(
      storagePath: string,
      expiresInSeconds: number = 3600
    ): Promise<string> {
      const supabase = getSupabaseClient();

      const { data, error } = await supabase.storage
        .from(BUCKET_NAME)
        .createSignedUrl(storagePath, expiresInSeconds);

      if (error) {
        throw new Error(`Failed to get signed URL: ${error.message}`);
      }

      return data.signedUrl;
    }

    /**
     * Download a document from storage as a Buffer.
     * Useful for passing to OCR APIs that need base64 input.
     */
    export async function downloadDocument(storagePath: string): Promise<Buffer> {
      const supabase = getSupabaseClient();

      const { data, error } = await supabase.storage
        .from(BUCKET_NAME)
        .download(storagePath);

      if (error) {
        throw new Error(`Failed to download document: ${error.message}`);
      }

      const arrayBuffer = await data.arrayBuffer();
      return Buffer.from(arrayBuffer);
    }
    ```

    3. Create `src/lib/supabase/documents.ts` — CRUD operations for the documents table:

    ```typescript
    import { getSupabaseClient } from './client';

    // Database row type (matches the SQL migration)
    export interface DocumentRow {
      id: string;
      case_id: string;
      type: string;
      file_name: string;
      file_path: string | null;
      file_url: string | null;
      ocr_raw_text: string | null;
      extracted_data: Record<string, unknown> | null;
      overall_confidence: number | null;
      processing_status: 'pending' | 'processing' | 'completed' | 'failed';
      processing_error: string | null;
      processing_time_ms: number | null;
      warnings: string[] | null;
      created_at: string;
      updated_at: string;
    }

    /**
     * Create a new document record (initially pending processing).
     */
    export async function createDocument(params: {
      case_id: string;
      type: string;
      file_name: string;
      file_path?: string;
      file_url?: string;
    }): Promise<DocumentRow> {
      const supabase = getSupabaseClient();

      const { data, error } = await supabase
        .from('documents')
        .insert({
          case_id: params.case_id,
          type: params.type,
          file_name: params.file_name,
          file_path: params.file_path || null,
          file_url: params.file_url || null,
          processing_status: 'pending',
        })
        .select()
        .single();

      if (error) {
        throw new Error(`Failed to create document: ${error.message}`);
      }

      return data as DocumentRow;
    }

    /**
     * Get a document by ID.
     */
    export async function getDocument(documentId: string): Promise<DocumentRow | null> {
      const supabase = getSupabaseClient();

      const { data, error } = await supabase
        .from('documents')
        .select('*')
        .eq('id', documentId)
        .single();

      if (error) {
        if (error.code === 'PGRST116') return null; // Not found
        throw new Error(`Failed to get document: ${error.message}`);
      }

      return data as DocumentRow;
    }

    /**
     * Get all documents for a case.
     */
    export async function getDocumentsByCase(caseId: string): Promise<DocumentRow[]> {
      const supabase = getSupabaseClient();

      const { data, error } = await supabase
        .from('documents')
        .select('*')
        .eq('case_id', caseId)
        .order('created_at', { ascending: true });

      if (error) {
        throw new Error(`Failed to get documents: ${error.message}`);
      }

      return (data || []) as DocumentRow[];
    }

    /**
     * Update a document with OCR results and extracted data.
     */
    export async function updateDocumentResults(
      documentId: string,
      results: {
        ocr_raw_text: string;
        extracted_data: Record<string, unknown>;
        overall_confidence: number;
        processing_status: 'completed' | 'failed';
        processing_error?: string;
        processing_time_ms: number;
        warnings?: string[];
      }
    ): Promise<DocumentRow> {
      const supabase = getSupabaseClient();

      const { data, error } = await supabase
        .from('documents')
        .update({
          ocr_raw_text: results.ocr_raw_text,
          extracted_data: results.extracted_data,
          overall_confidence: results.overall_confidence,
          processing_status: results.processing_status,
          processing_error: results.processing_error || null,
          processing_time_ms: results.processing_time_ms,
          warnings: results.warnings || [],
        })
        .eq('id', documentId)
        .select()
        .single();

      if (error) {
        throw new Error(`Failed to update document: ${error.message}`);
      }

      return data as DocumentRow;
    }

    /**
     * Update document processing status (used to mark as 'processing' before OCR).
     */
    export async function updateDocumentStatus(
      documentId: string,
      status: 'pending' | 'processing' | 'completed' | 'failed'
    ): Promise<void> {
      const supabase = getSupabaseClient();

      const { error } = await supabase
        .from('documents')
        .update({ processing_status: status })
        .eq('id', documentId);

      if (error) {
        throw new Error(`Failed to update document status: ${error.message}`);
      }
    }
    ```

    IMPORTANT: Create the Supabase client factory as a function (not a singleton) because Next.js API routes run in serverless contexts. The service role key is preferred for server-side operations (bypasses RLS), falling back to anon key.
  </action>
  <verify>
    - `npx tsc --noEmit` passes — all Supabase helpers compile
    - `src/lib/supabase/client.ts` exports getSupabaseClient
    - `src/lib/supabase/storage.ts` exports uploadDocument, getDocumentSignedUrl, downloadDocument
    - `src/lib/supabase/documents.ts` exports createDocument, getDocument, getDocumentsByCase, updateDocumentResults, updateDocumentStatus
    - No hardcoded credentials — all from environment variables
  </verify>
  <done>
    Supabase database migration for documents table created. Document CRUD helpers and storage upload/download functions built. The data layer is ready for the document processing pipeline.
  </done>
</task>

</tasks>

<verification>
1. `npx tsc --noEmit` passes with no type errors
2. SQL migration file is valid and creates documents table with all required columns
3. Supabase client, storage, and document helpers all compile and export correctly
4. No hardcoded secrets in any file
5. Document CRUD supports the full lifecycle: create (pending) -> update status (processing) -> update results (completed/failed)
</verification>

<success_criteria>
- Documents table schema covers all needed columns for OCR output, extracted data, and confidence scores
- File upload/download to Supabase Storage works with signed URLs
- Document CRUD operations support the processing lifecycle (pending -> processing -> completed/failed)
- All code compiles with zero TypeScript errors
</success_criteria>

<output>
After completion, create `.planning/phases/03-document-processing/03-02-SUMMARY.md`
</output>
